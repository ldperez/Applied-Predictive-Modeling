---
title: "ADS503_UPham_team5"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
**Column Descriptions**
HeartDisease: Respondents that have ever reported having coronary heart disease (CHD) or myocardial infarction (MI).
BMI: Body Mass Index (BMI).
Smoking: Have you smoked at least 100 cigarettes in your entire life?
AlcoholDrinking: Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week
Stroke: (Ever told) (you had) a stroke?
PhysicalHealth: Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good? (0-30 days).
MentalHealth: Thinking about your mental health, for how many days during the past 30 days was your mental health not good? (0-30 days).
DiffWalking: Do you have serious difficulty walking or climbing stairs?
Sex: Are you male or female?
AgeCategory: Fourteen-level age category. (then calculated the mean)
Race: Imputed race/ethnicity value.
Diabetic: (Ever told) (you had) diabetes?
PhysicalActivity: Adults who reported doing physical activity or exercise during the past 30 days other than their regular job.
GenHealth: Would you say that in general your health is...
SleepTime: On average, how many hours of sleep do you get in a 24-hour period?
Asthma: (Ever told) (you had) asthma?
KidneyDisease: Not including kidney stones, bladder infection or incontinence, were you ever told you had kidney disease?
SkinCancer: (Ever told) (you had) skin cancer?

```{r}
library(ggplot2)
library(lattice)
library(caret)
library(e1071)
data(tecator)
library(survival)
library(Hmisc)
library(corrplot)
```


```{r}
heart <- read.csv("heart.csv")
str(heart)
```

## Duplicates

```{r}
#Total duplicates
sum(duplicated(heart))
```

```{r}
#Observe the duplicates
dup <- heart[duplicated(heart),]
head(dup)
```

```{r}
#check the balance in original data and in duplicates
table(heart$HeartDisease)
table(dup$HeartDisease)
```

**There are no IDs to identify whether these duplicates are real, but re the chances of duplicates are low. It's better to removed them.**

```{r}
#Remove duplicates
heartRedDup<- unique(heart)
dim(heartRedDup)
```

## 1. Define the scope of the model:

### The model will be built to predict heart disease for race of White, Black and Hispanic

```{r}
unique(heartRedDup$Race)
```

```{r}
#Subset the data that contain White, Black and Hispanic
heartRedRace <- subset (heartRedDup, Race == "White" | Race== "Black" | 
                          Race=="Hispanic")
dim(heartRedRace)
unique(heartRedRace$Race)
```

### The model will be built based on average SleepTime 3-12hr/24 hrs. Since people who sleep less or more than this range might have other serious health issues that might not been report and can affect the model

```{r}
#Subset the data to have sleep average from 4-16hrs
heartRedRaceSleep <- subset(heartRedRace, heartRedRace$SleepTime >= 4 & heartRedRace$SleepTime <= 16 )
dim(heartRedRaceSleep)
```

### Re-categorize diabetic categories in "yes and "no and drop "Yes (during pregnancy)". Since diabetic during pregnancy mostly temporary an d only relevant for women

```{r}
table(heartRedRaceSleep$Diabetic)
```

```{r}
#Drop rows with "Yes (during pregnancy)"
heartRedRaceSleepDi <- subset(heartRedRaceSleep, heartRedRaceSleep$Diabetic!= "Yes (during pregnancy)")
#Re-categorize diabetic categories in "yes and "no considering borderline and pregnacy are "no"
heartRedRaceSleepDi$Diabetic <- ifelse(heartRedRaceSleepDi$Diabetic== "Yes", "Yes", "No")
unique(heartRedRaceSleepDi$Diabetic)
```

## Missing data

```{r}
colSums(is.na(heartRedRaceSleepDi))
```

**--> There is no missing data ** 

## Outliers

```{r}
par(mfrow = c(2, 2))
boxplot(heartRedRaceSleepDi$BMI, main="BMI")
boxplot(heartRedRaceSleepDi$PhysicalHealth, main="Physical Health")
boxplot(heartRedRaceSleepDi$MentalHealth, main="Mental Health")
boxplot(heartRedRaceSleepDi$SleepTime, main="Sleep Time")
```

## Feature engineering

### Change ordinal catergorical features to ordered factors using label-coding
### Change dependent features normial factors using label-coding

```{r}
#check values of ordinal features
unique(heartRedRaceSleepDi$AgeCategory)
unique(heartRedRaceSleepDi$GenHealth)
```

```{r}
#Convert Ordinal category variable into factors with order
heartRedRaceSleepDi$AgeCategory <- factor(heartRedRaceSleepDi$AgeCategory, 
                                        ordered = TRUE)
heartRedRaceSleepDi$GenHealth   <- factor(heartRedRaceSleepDi$GenHealth, 
                            levels = c("Poor", "Fair", "Good", 
                                       "Very good", "Excellent"),
                            ordered = TRUE)

#Check the conversion order
min(heartRedRaceSleepDi$AgeCategory)
min(heartRedRaceSleepDi$GenHealth)

#convert dependent variable to factor using label-encoding
heartRedRaceSleepDi$HeartDisease <- factor(heartRedRaceSleepDi$HeartDisease)
```


**Sepearate predictors and dependent variable**

```{r}
heartRedRaceSleepDiX <- heartRedRaceSleepDi [,-1]
heartRedRaceSleepDiY <- subset(heartRedRaceSleepDi, select=HeartDisease)
dim(heartRedRaceSleepDiY)
```

###Convert factors into numeric

```{r}
#Get Column with factor type 
toConvert <- sapply(heartRedRaceSleepDiX, is.factor)
#Display numeric values of factor variables
heartRedRaceSleepDiX[, toConvert] <- sapply(heartRedRaceSleepDiX[, toConvert], unclass)
str(heartRedRaceSleepDiX)
```

## Training/Test split

**Since the some of the columns are very imbalance including the response variables. stratified split would be use to ensure similar distribution of classes in both training and test sets.**

```{r plitting data}
set.seed(23)
# Create random stratified sample splits 
trainingRows <- createDataPartition(heartRedRaceSleepDiY$HeartDisease, p = .9, list = FALSE)
#subset the data into object for training using
trainX <- heartRedRaceSleepDiX[trainingRows,]
trainY <- heartRedRaceSleepDiY[trainingRows,]
trainY <- as.data.frame(trainY)
#subset the data into object for test using
testX <- heartRedRaceSleepDiX[-trainingRows,]
testY <- heartRedRaceSleepDiY[-trainingRows,]
testY <- as.data.frame(testY)
dim(trainX)
dim(testX)
```

## Explore data in training set

```{r}
summary(trainX)
```

```{r, some histogram}
# some histograms
par(mfrow = c(1,3))
hist(trainX$BMI)
hist(trainX$PhysicalHealth)
hist(trainX$SleepTime)
```


```{r} 
hist.data.frame(trainX)
```

### Check for highly correalted predictors

```{r highly correlated predicotrs}
#Extract numeric features
trainXNum <-  sapply(trainX, is.numeric)
#correlation of the numeric features
correlations <- cor(trainX[, trainXNum])
highCorr <- findCorrelation(correlations, cutoff = .75) 
length(highCorr)
```

**--> No high correlation between numeric variables**
### Heat map

```{r correaltion heatmap}
#Create correlation matrix
heartTrainCor = cor(trainX[, trainXNum])
#plot cor matrix
corrplot(heartTrainCor, method="color", addCoef.col= 1, number.cex = 0.4)
```

**-> correlations are in reasonable range**

### Check for zero variance predictors

```{r}
degeneratecols <- nearZeroVar(trainX)
degeneratecols
colnames(trainX[degeneratecols])
```

```{r}
#Exclude high variance variable from training and test sets
trainXFil <- trainX[, -degeneratecols]
testXFil <- testX[, -degeneratecols]
dim(trainXFil)
```

**--> Exclude near-zero variance feature: "Stroke" and "KidneyDisease"**

### Check realtionship of independent variables and dependent variable

```{r}
#Compare Race with Heart disease overlayed
par(mfrow = c(1,2))
ggplot(trainXFil, aes(Race)) + geom_bar(aes(fill = trainY$trainY))
ggplot(trainXFil, aes(Race)) + geom_bar(aes(fill = trainY$trainY), position = "fill")

```


```{r}
#Compare SleepTime with Heart disease overlayed
par(mfrow = c(1,2))
ggplot(trainXFil, aes(SleepTime)) + geom_bar(aes(fill = trainY$trainY))
ggplot(trainXFil, aes(SleepTime)) + geom_bar(aes(fill = trainY$trainY), position = "fill")
```


**-> sleep time seems to have some correlation with  heart disease: where sleep time less than 5 and higher than 8 or 9hrs


```{r}
#Compare Age with Heart disease overlayed
par(mfrow = c(1,2))
ggplot(trainXFil, aes(AgeCategory)) + geom_bar(aes(fill = trainY$trainY))
ggplot(trainXFil, aes(AgeCategory)) + geom_bar(aes(fill = trainY$trainY), position = "fill")
```
**AgeCategory showed linear correlation with HeartDisease**

```{r}
#Compare Sleep with MentalHealth
par(mfrow = c(1,3))
hist(trainXFil$MentalHealth)
hist(trainXFil$SleepTime)
hist(trainXFil$PhysicalHealth)
```

Check
trainXFil[trainX$MentalHealth >= 20,]
trainXFil[trainX$PhysicalHealth >= 20,]

```{r}
# BMI distribution
hist(trainX$BMI)
```

```{r}
#scatter plot Genhealth vs PhysicalHealth
plot(trainXFil$SleepTime,trainXFil$PhysicalHealth, main="GenHealth Vs. PhysicalHealth",
   xlab="GenHealth ", ylab="PhysicalHealth", pch=19)
```
```{r}

```


### One-hot encoding the nominal features

```{r}
#Convert nominal predictors into dummies
dummy <- dummyVars(~., data = trainXFil, fullRank = T)
trainXFilDum<- data.frame(predict(dummy, newdata= trainXFil))
head(trainXFilDum)

testXFilDum<- data.frame(predict(dummy, newdata= testXFil))
dim(trainXFilDum)
```

## Combine train predictors and response variables
## Underresampling the data since the response is very imbalance and the dataset is too large to do overresampling

```{r}
set.seed(100)
#Combine train predictors and response variables
trainCom <- cbind(trainXFilDum, trainY)
colnames(trainCom)[17] <- "HeartDisease" #change response variable name back to HeartDisease

testFinal <- cbind(testXFilDum, testY)
colnames(testFinal)[17]<- "HeartDisease"

#Downsampling the data
trainFinal <- downSample(x =trainCom[,1:16], y= trainCom$HeartDisease, yname= "HeartDisease" )
dim(trainFinal)
dim(testFinal)
head(trainFinal)
head(testFinal)

```

```{r}
head(trainFinal)
head(testFinal)
```

```#{r saving Final dataframes}
write.csv(trainFinal,"/Users/lamnguyen/Dropbox/Uyen bom/USD/ADS503/Final Project/Github/trainFinal.csv", row.names = FALSE)
write.csv(testCom,"/Users/lamnguyen/Dropbox/Uyen bom/USD/ADS503/Final Project/Github/testCom.csv", row.names = FALSE)
```
**-->Save dataframe as RDS file instead of csv to preserve data type of factors**

```{r}
saveRDS(trainFinal, file="/Users/lamnguyen/Dropbox/Uyen bom/USD/ADS503/Final Project/Github/Applied-Predictive-Modeling/trainFinal.Rds")
saveRDS(testFinal, file="/Users/lamnguyen/Dropbox/Uyen bom/USD/ADS503/Final Project/Github/Applied-Predictive-Modeling/testFinal.Rds")
```

# RESTART HERE

```{r}
trainFinal <- readRDS("/Users/lamnguyen/Dropbox/Uyen bom/USD/ADS503/Final Project/Github/Applied-Predictive-Modeling/trainFinal.Rds")
testCom <- readRDS("/Users/lamnguyen/Dropbox/Uyen bom/USD/ADS503/Final Project/Github/Applied-Predictive-Modeling/testFinal.Rds")
```

```{r}
dim(trainFinal)
dim(testFinal)
head(trainFinal,2)
head(testFinal,2)
```


```{r}
library(ggplot2)
library(lattice)
library(caret)
library(e1071)
data(tecator)
library(survival)
library(Hmisc)
library(corrplot)
```

```{r}
table(trainFinal$HeartDisease)
```

## Build Classification models

**Logistic Regression **

```{r}
set.seed(23)
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)
```

```{r}
set.seed(23)
lrFit<- train(x = trainFinal[, 1:16], 
               y = trainFinal$HeartDisease,
               method = "glm",
               metric= "Spec",
               trControl = ctrl)
```


```{r}
lrFit
```

```{r}
#Create dataframe for results
testResults <- data.frame(obs = testCom$HeartDisease,
                          LR = predict(lrFit, testFinal[, 1:16]))
```

```{r}
head(testResults)
```


```{r}
confusionMatrix(testResults$LR, testResults$obs, positive = "No")
```

```{r}
lrImp <- varImp(lrFit, scale = FALSE)
plot(lrImp)
```

**Linear Discriminant Analysis ** 

```{r}
set.seed(23)
ldaFit <- train(x =  trainFinal[, 1:16], 
                y = trainFinal$HeartDisease,
                method = "lda",
                preProc = c("center","scale"),
                metric ="Spec",
                trControl = ctrl)

testResults$LDA <- predict(ldaFit, testFinal[, 1:16])
```

```{r}
ldaFit
```

```{r}
testResults$LDA <- predict(ldaFit, testFinal[,1:16])

confusionMatrix(testResults$LDA, testResults$obs, positive = "No")
```

**Neural Network**

```{r}
set.seed(23)
#set number of hidden units and decay parameters that controls regularization
nnetGrid <- expand.grid(size=1:3, decay=c(0,0.1,0.2,0.3,0.4,0.5,1,2))

nnetFit <- train(x = trainFinal[, 1:16], 
                y = trainFinal$HeartDisease,
                method = "nnet",
                metric = "Spec",
                preProc = c("center", "scale", "spatialSign"),
                tuneGrid = nnetGrid,
                trace = FALSE, 
                maxit = 1000, 
                trControl = ctrl)

```


```{r}
nnetFit
```

```{r}
testResults$nnet <- predict(nnetFit, testCom[,1:16])
confusionMatrix(testResults$nnet, testResults$obs, positive = "No")
```

```{r}
nnetImp <- varImp(nnetFit, scale = FALSE)
plot(nnetImp)
```

**Naive Bayes**
**--> variable have to be factors. Dummies could be will be problematic in estimating probability**
```{r}
#install.packages("klaR")
library(klaR)
```

```{r}

```


**Penalized Logistic Regression **

```{r}
glmnGrid <- expand.grid(alpha = c(0,  .1,  .2, .4, .6, .8, 1),
                        lambda = seq(.01, .2, length = 10))
set.seed(100)
glmnFit <- train(x = trainFinal[,1:16], 
                 y = trainFinal$HeartDisease,
                 method = "glmnet",
                 tuneGrid = glmnGrid,
                 preProc = c("center", "scale"),
                 metric = "Spec",
                 trControl = ctrl)
#glmnFit
testResults$glmnet <- predict(glmnFit, testCom[,1:16])
```


```{r}
glmnFit
```

```{r}
testResults$Glmn <- predict(glmnFit, testCom[,1:16])

confusionMatrix(testResults$Glmn, testResults$obs, positive = "No")
```


```{r}
set.seed(23)
nscFit <- train(x = trainFinal[, 1:16], 
                y = trainFinal$HeartDisease,
                method = "pam",
                preProc = c("center", "scale"),
                tuneGrid = data.frame(threshold = seq(0, 25, length = 30)),
                metric = "Spec",
                trControl = ctrl)
```


```{r}
nscFit
```

```{r}
testResults$NSC <- predict(nscFit, testCom[, 1:16])
confusionMatrix(testResults$NSC, testResults$obs, positive = "No")
```

**Mixture Discriminant Analysis**

```{r}
set.seed(23)

mdaFit <- train(x = trainFinal[, 1:16], 
                y = trainFinal$HeartDisease,
               method = "mda",
               preProc = c("center", "scale"),
               tuneGrid = expand.grid(subclasses=1:3),
               metric = "Spec",
               trControl = ctrl)
mdaFit
```

```{r}
testResults$MDA <- predict(mdaFit, testCom[, 1:16])
confusionMatrix(testResults$MDA, testResults$obs, positive = "No")
```


**Support Vector Machine Learning**

```{r}
library(kernlab)
library(arules)
```

```{r}
sigmaEst <- kernlab::sigest(as.matrix(trainFinal[, 1:16]))
svmgrid <- expand.grid(sigma = sigmaEst, C = 2^seq(-2,+2))

set.seed(23)
svmRFit <- train(x = trainFinal[,1:16], 
                y = trainFinal$HeartDisease,
                method = "svmRadial",
                tuneGrid = svmgrid,
                metric = "ROC",
                preProc = c("center", "scale"),
                trControl = ctrl)

```


```{r}
svmRFit
```

```{r}
testResults$SVM <- predict(svmRFit, testCom[,1:16])
confusionMatrix(testResults$SVM, testResults$obs, positive = "No")
```

```{r}
set.seed(100)
knnFit <- train(x = trainFinal[,1:25], 
                y = trainFinal$HeartDisease,
                method = "knn",
                preProc = c("center", "scale"),
                tuneLength = 20,
                metric = "ROC",
                trControl = ctrl)
knnFit
```


```{r}
testResults$knn <- predict(knnFit, testCom[,1:25])
```

```{r}
confusionMatrix(testResults$knn, testResults$obs, positive = "No")
```

**Decision tree**

```{r}
# set.seed(100)
# #Use categorical features instead of dummies
# rpartFit <- train(x = trainFinal[, 1:25], 
#                 y = trainFinal$HeartDisease,
#                 method = "rpart",
#                 tuneLength = 30,
#                 metric = "ROC",
#                 trControl = ctrl)
# rpartFit
```


```{r}
testResults$rpart <- predict(knnFit, testCom[,1:25])
confusionMatrix(testResults$rpart, testResults$obs, positive = "No")
```


```{r}
rpartImp <- varImp(rpartFit, scale = FALSE)
plot(rpartImp)
```

**Random Forest**

```{r}
mtryValues <- seq(1,10,1)

set.seed(100)

rfFit <- train(x = trainFinal[,1:25], 
                y = trainFinal$HeartDisease,
                method = "rf",
                ntree = 1000,
                tuneGrid = data.frame(mtry = mtryValues),
                metric = "ROC",
                trControl = ctrl)

```


```{r}
```


```{r}
```


```{r}
```

